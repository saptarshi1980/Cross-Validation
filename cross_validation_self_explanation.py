# -*- coding: utf-8 -*-
"""Cross_Validation_Self_explanation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sEIB-Pu_wb5_X31MUmk1Ubtv9qtTvpXl

### Cross-validation is a statistical method used in machine learning to evaluate the performance of a model. It involves partitioning the data into subsets, training the model on some subsets, and testing it on the remaining subsets. This helps in assessing how the model will generalize to an independent dataset. Here are various cross-validation techniques, along with their advantages, disadvantages, and Python code examples

## K-Fold Cross-Validation
Description
The dataset is divided into k equally sized folds. The model is trained k times, each time using k-1 folds for training and the remaining fold for testing.

Advantages
Reduces bias as every data point gets to be in a test set exactly once and in a training set k-1 times.
Reduces variance as the performance measure is averaged over k different training and test sets.
Disadvantages
Computationally expensive for large k values.
Not suitable for very small datasets.
"""

from sklearn.model_selection import KFold
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import numpy as np

# Sample data
X = np.array([[i] for i in range(10)])
y = np.array([2*i + 1 for i in range(10)])

kf = KFold(n_splits=5)
model = LinearRegression()
mse_scores = []

for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse_scores.append(mean_squared_error(y_test, y_pred))

print(f'Mean MSE: {np.mean(mse_scores)}')

"""## kf.split(X) splits the data into 5 folds, providing train_index and test_index for each iteration.
## X_train and X_test are created by indexing X with train_index and test_index, respectively.
## y_train and y_test are created by indexing y with train_index and test_index, respectively.
## The model is trained using X_train and y_train.
## Predictions (y_pred) are made using X_test.
## The mean squared error between y_test and y_pred is calculated and appended to mse_scores.
"""

for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    print(f'Train Index: {train_index}')
    print(f'Test Index: {test_index}')
    print(



"""# Stratified K-Fold Cross-Validation
##Description
## Similar to K-Fold but ensures that each fold has the same proportion of class labels as the original dataset. This is particularly useful for imbalanced datasets.

## Advantages
Maintains the distribution of classes in each fold.
Better for classification problems with imbalanced classes.
Disadvantages
Computationally expensive for large k values.
Not suitable for very small datasets.
"""

from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Sample data
X = np.array([[i] for i in range(10)])
y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])  # Imbalanced classes

skf = StratifiedKFold(n_splits=5)
model = LogisticRegression()
accuracy_scores = []

for train_index, test_index in skf.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy_scores.append(accuracy_score(y_test, y_pred))

print(f'Mean Accuracy: {np.mean(accuracy_scores)}')

"""### Leave-One-Out Cross-Validation (LOOCV)
## Description
## A special case of k-fold cross-validation where k equals the number of data points in the dataset. Each observation is used as a single test set while the remaining observations form the training set.

##Advantages
### Uses maximum data for training in each iteration.
Very unbiased because each data point is tested once.
Disadvantages
## Extremely computationally expensive for large datasets.
High variance since each training set is nearly identical.
"""

from sklearn.model_selection import LeaveOneOut
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Sample data
X = np.array([[i] for i in range(10)])
y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])

loo = LeaveOneOut()
model = DecisionTreeClassifier()
accuracy_scores = []

for train_index, test_index in loo.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy_scores.append(accuracy_score(y_test, y_pred))

print(f'Mean Accuracy: {np.mean(accuracy_scores)}')

"""## Time Series Split (Rolling Cross-Validation)
# Description
## Used for time series data where the order of data points is important. The dataset is split into training and test sets based on a rolling window approach.

## Advantages

# Preserves the temporal order of data points.
# Useful for time series forecasting problems.
# Disadvantages
# Limited to time series data.
# Can be less stable if the time series has strong trends or seasonalities.
"""

from sklearn.model_selection import TimeSeriesSplit
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error

# Sample data
X = np.array([[i] for i in range(10)])
y = np.array([2*i + 1 for i in range(10)])

tscv = TimeSeriesSplit(n_splits=5)
model = SVR()
mae_scores = []

for train_index, test_index in tscv.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mae_scores.append(mean_absolute_error(y_test, y_pred))

print(f'Mean MAE: {np.mean(mae_scores)}')

"""# Randomized Search Cross-Validation
# Description
# Combines cross-validation with random search of hyperparameter tuning. It randomly samples hyperparameters and evaluates the model performance using cross-validation.

# Advantages
# Efficient for hyperparameter tuning.
# Can handle a large search space.
# Disadvantages
# Requires a large number of iterations to be effective.
# Computationally expensive for complex models.
"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier

# Sample data
X = np.random.rand(100, 5)
y = np.random.randint(0, 2, size=100)

param_dist = {
    'n_estimators': [10, 50, 100],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

model = RandomForestClassifier()
random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5)
random_search.fit(X, y)

print(f'Best Parameters: {random_search.best_params_}')
print(f'Best Score: {random_search.best_score_}')