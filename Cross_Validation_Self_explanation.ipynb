{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-validation is a statistical method used in machine learning to evaluate the performance of a model. It involves partitioning the data into subsets, training the model on some subsets, and testing it on the remaining subsets. This helps in assessing how the model will generalize to an independent dataset. Here are various cross-validation techniques, along with their advantages, disadvantages, and Python code examples"
      ],
      "metadata": {
        "id": "lu9AqeZHBhmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Fold Cross-Validation\n",
        "Description\n",
        "The dataset is divided into k equally sized folds. The model is trained k times, each time using k-1 folds for training and the remaining fold for testing.\n",
        "\n",
        "Advantages\n",
        "Reduces bias as every data point gets to be in a test set exactly once and in a training set k-1 times.\n",
        "Reduces variance as the performance measure is averaged over k different training and test sets.\n",
        "Disadvantages\n",
        "Computationally expensive for large k values.\n",
        "Not suitable for very small datasets."
      ],
      "metadata": {
        "id": "xEWF2J3HBr6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[i] for i in range(10)])\n",
        "y = np.array([2*i + 1 for i in range(10)])\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "model = LinearRegression()\n",
        "mse_scores = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse_scores.append(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(f'Mean MSE: {np.mean(mse_scores)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElHzPWgjBs6m",
        "outputId": "a5bcfaca-c80d-4ea1-959f-5e1492b23e19"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean MSE: 8.993014319519535e-30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## kf.split(X) splits the data into 5 folds, providing train_index and test_index for each iteration.\n",
        "## X_train and X_test are created by indexing X with train_index and test_index, respectively.\n",
        "## y_train and y_test are created by indexing y with train_index and test_index, respectively.\n",
        "## The model is trained using X_train and y_train.\n",
        "## Predictions (y_pred) are made using X_test.\n",
        "## The mean squared error between y_test and y_pred is calculated and appended to mse_scores."
      ],
      "metadata": {
        "id": "Fqb8cpqeEO5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    print(f'Train Index: {train_index}')\n",
        "    print(f'Test Index: {test_index}')\n",
        "    print("
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH4I-nosCqKd",
        "outputId": "08e6a1c1-173a-4189-ecd9-a573fdb1a256"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Index: [2 3 4 5 6 7 8 9]\n",
            "Test Index: [0 1]\n",
            "X_train: [[2]\n",
            " [3]\n",
            " [4]\n",
            " [5]\n",
            " [6]\n",
            " [7]\n",
            " [8]\n",
            " [9]]\n",
            "X_test: [[0]\n",
            " [1]]\n",
            "y_train: [ 5  7  9 11 13 15 17 19]\n",
            "y_test: [1 3]\n",
            "Train Index: [0 1 4 5 6 7 8 9]\n",
            "Test Index: [2 3]\n",
            "X_train: [[0]\n",
            " [1]\n",
            " [4]\n",
            " [5]\n",
            " [6]\n",
            " [7]\n",
            " [8]\n",
            " [9]]\n",
            "X_test: [[2]\n",
            " [3]]\n",
            "y_train: [ 1  3  9 11 13 15 17 19]\n",
            "y_test: [5 7]\n",
            "Train Index: [0 1 2 3 6 7 8 9]\n",
            "Test Index: [4 5]\n",
            "X_train: [[0]\n",
            " [1]\n",
            " [2]\n",
            " [3]\n",
            " [6]\n",
            " [7]\n",
            " [8]\n",
            " [9]]\n",
            "X_test: [[4]\n",
            " [5]]\n",
            "y_train: [ 1  3  5  7 13 15 17 19]\n",
            "y_test: [ 9 11]\n",
            "Train Index: [0 1 2 3 4 5 8 9]\n",
            "Test Index: [6 7]\n",
            "X_train: [[0]\n",
            " [1]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [5]\n",
            " [8]\n",
            " [9]]\n",
            "X_test: [[6]\n",
            " [7]]\n",
            "y_train: [ 1  3  5  7  9 11 17 19]\n",
            "y_test: [13 15]\n",
            "Train Index: [0 1 2 3 4 5 6 7]\n",
            "Test Index: [8 9]\n",
            "X_train: [[0]\n",
            " [1]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [5]\n",
            " [6]\n",
            " [7]]\n",
            "X_test: [[8]\n",
            " [9]]\n",
            "y_train: [ 1  3  5  7  9 11 13 15]\n",
            "y_test: [17 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5qD97VwEi_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stratified K-Fold Cross-Validation\n",
        "##Description\n",
        "## Similar to K-Fold but ensures that each fold has the same proportion of class labels as the original dataset. This is particularly useful for imbalanced datasets.\n",
        "\n",
        "## Advantages\n",
        "Maintains the distribution of classes in each fold.\n",
        "Better for classification problems with imbalanced classes.\n",
        "Disadvantages\n",
        "Computationally expensive for large k values.\n",
        "Not suitable for very small datasets."
      ],
      "metadata": {
        "id": "67F4KeHgElN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hTIc9D-DJ_F3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[i] for i in range(10)])\n",
        "y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])  # Imbalanced classes\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "model = LogisticRegression()\n",
        "accuracy_scores = []\n",
        "\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print(f'Mean Accuracy: {np.mean(accuracy_scores)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8h_45HOEsa4",
        "outputId": "32461cd9-daa7-4581-a0ac-7e5f2c84be6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Leave-One-Out Cross-Validation (LOOCV)\n",
        "## Description\n",
        "## A special case of k-fold cross-validation where k equals the number of data points in the dataset. Each observation is used as a single test set while the remaining observations form the training set.\n",
        "\n",
        "##Advantages\n",
        "### Uses maximum data for training in each iteration.\n",
        "Very unbiased because each data point is tested once.\n",
        "Disadvantages\n",
        "## Extremely computationally expensive for large datasets.\n",
        "High variance since each training set is nearly identical."
      ],
      "metadata": {
        "id": "H2vRI1ZQKCN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[i] for i in range(10)])\n",
        "y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n",
        "\n",
        "loo = LeaveOneOut()\n",
        "model = DecisionTreeClassifier()\n",
        "accuracy_scores = []\n",
        "\n",
        "for train_index, test_index in loo.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print(f'Mean Accuracy: {np.mean(accuracy_scores)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Lv7b_9PKA9O",
        "outputId": "c40e8cd5-eb37-437c-b54c-08aaa1ab78f3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YgHnreIOKlIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time Series Split (Rolling Cross-Validation)\n",
        "# Description\n",
        "## Used for time series data where the order of data points is important. The dataset is split into training and test sets based on a rolling window approach.\n",
        "\n",
        "## Advantages\n",
        "\n",
        "# Preserves the temporal order of data points.\n",
        "# Useful for time series forecasting problems.\n",
        "# Disadvantages\n",
        "# Limited to time series data.\n",
        "# Can be less stable if the time series has strong trends or seasonalities."
      ],
      "metadata": {
        "id": "r0YX3zutKnqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[i] for i in range(10)])\n",
        "y = np.array([2*i + 1 for i in range(10)])\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "model = SVR()\n",
        "mae_scores = []\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
        "\n",
        "print(f'Mean MAE: {np.mean(mae_scores)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfMoo48tKTvu",
        "outputId": "d230c66f-635e-4f95-ab8c-9de5d2f34ca8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean MAE: 6.751543153635419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Randomized Search Cross-Validation\n",
        "# Description\n",
        "# Combines cross-validation with random search of hyperparameter tuning. It randomly samples hyperparameters and evaluates the model performance using cross-validation.\n",
        "\n",
        "# Advantages\n",
        "# Efficient for hyperparameter tuning.\n",
        "# Can handle a large search space.\n",
        "# Disadvantages\n",
        "# Requires a large number of iterations to be effective.\n",
        "# Computationally expensive for complex models."
      ],
      "metadata": {
        "id": "sKnj1fyHKyZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Sample data\n",
        "X = np.random.rand(100, 5)\n",
        "y = np.random.randint(0, 2, size=100)\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5)\n",
        "random_search.fit(X, y)\n",
        "\n",
        "print(f'Best Parameters: {random_search.best_params_}')\n",
        "print(f'Best Score: {random_search.best_score_}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ40UtMTKrCG",
        "outputId": "ae966bfe-3674-4192-a785-b9281fd65b73"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'n_estimators': 50, 'min_samples_split': 5, 'max_depth': 10}\n",
            "Best Score: 0.63\n"
          ]
        }
      ]
    }
  ]
}